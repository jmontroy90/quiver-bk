{
  "title": "Encoding and Evolution",
  "cells": [
    {
      "type": "markdown",
      "data": "##### Formats for Encoding Data\nData in memory is stored in your typical lists, structs, arrays, and so on - these data structures are implemented by your language of choice (or some runtime environment like the JVM) to be ideal for CPU access and (hopefully) cache locality. Pointers are used heavily.\n\nWhen sending data out over a network, you need a different representation (like JSON, Avro, Protobuf, Thrift). Going from a memory representation to these wire formats is called (one of): `encoding, serialization, marshalling`, and going back to the memory format from a wire format is `decoding, parsing, deserializing, unmarshalling`.\n\nLanguages offer their own serialization formats (`java.io.Serializable`, Python's `pickle`, Ruby's `Marshall`), but these are obviously not ideal for language interop, and frequently yield security problems due ot the necessity of unmarshalling into potentially arbitrary classes. Some of them just aren't performant, and versioning is MIA.\n\nStandard wire formats can help here - `XML, JSON, CSV`. But these all have their own deficiencies. JSON and XML are both bloated (XML moreso), and CSV is brittle. JSON can't distinguish integers from floats, and neither XML or JSON support binary strings. JSON has to provide data fields with every message, and isn't actually working with that schema - it's just somewhat vague bloat.\n\nBinary formats have emerged for XML and JSON, but both still include field names and don't really save that much space. And obviously lose human readability.\n\nThrift and Protobuf have arisen as schematized encoders - example IDL for Thrift and then Protobuf:\n\n```\nstruct Person {\n  1: required string       userName,\n  2: optional i64          favoriteNumber,\n  3: optional list<string> interests\n}\n```\n\n```\nmessage Person {\n    required string user_name       = 1;\n    optional int64  favorite_number = 2;\n    repeated string interests       = 3;\n}\n```\n\nThrift has two formats - `BinaryProtocol` and `CompactProtocol`. Protobuf just has one that is closest to `BinaryProtocol`. \n\n\n### Other notes:\n\n_Class injection_: \"In order to restore data in the same object types, the decoding process needs to be able to instantiate arbitrary classes\"\n\n_Deserializing_: Language-specific serialization is slow, usually. Java is notorious.\n\n\n### Avro: Reader vs Writer Schema\nThere are a couple of use cases for how to make the Avro writer schema available for records being processed:\n\n1) **Large file with lots of records** - a common use for Avro—especially in the context of Hadoop—is for storing a large file containing millions of records, all encoded with the same schema. (We will discuss this kind of situation in Chapter 10.) In this case, the writer of that file can just include the writer’s schema once at the beginning of the file. Avro specifies a file format (object container files) to do this.\n\n2) **Database with individually written records** - in a database, different records may be written at different points in time using different writer’s schemas—you cannot assume that all the records will have the same schema. The simplest solution is to include a version number at the beginning of every encoded record, and to keep a list of schema versions in your database. A reader can fetch a record, extract the version number, and then fetch the writer’s schema for that version number from the database. Using that writer’s schema, it can decode the rest of the record.\n\n3) **Sending records over a network connection** - when two processes are communicating over a bidirectional network connection, they can negotiate the schema version on connection setup and then use that schema for the lifetime of the connection.\n\nAvro fields are matched by name in the schemas for evolution - this obviates the need for numbering your fields, and allows for dynamic schema generation and comparison.\n\n> By contrast, if you were using Thrift or Protocol Buffers for this purpose, the field tags would likely have to be assigned by hand: every time the database schema changes, an administrator would have to manually update the mapping from database column names to field tags. (It might be possible to automate this, but the schema generator would have to be very careful to not assign previously used field tags.)\n\nCode-gen is available in Avro, Thrift, and Protobuf. \n\n_RPC vs REST_ — Use something real for remote dataflow, like REST or SOAP (but not SOAP). RPC pretends it's just a local function call, but timeouts and network volatility can give you problems with lack of information, duplicated retry calls, and excessive serialized data sent over the network as arguments.\n"
    }
  ]
}