{
  "title": "Transactions",
  "cells": [
    {
      "type": "markdown",
      "data": "## General Overview of the Chapter\n1) For this chapter, let's ignore the complexities that come with actually distributed databases\n    1) there are enough concurrency + consistency issues that can come in a single-server DB\n    2) How transactional guarantees can be implemented in a distributed system will be addressed later\n2) **Transactions** are an abstraction for simplifying user code for dealing with consistency / reliability issues in a DB\n    * In general, implemented as a set of reads + writes that either succeed or fail together\n    * no need for the application to deal with **partial failures**\n3) Preconceptions about transactions:\n    1) The traditional view: Any 'serious database' must implement transactions and offers ACID guarantees\n    2) The NoSQL view: Transactions prevent scalability. 'Big data' applications cannot thrive in a world with strong transactional guarantees\n\n## ACID (Atomicity, Consistency, Isolation, Durability)\n* **Atomicity** - perhaps a better term is **Abortability**.  The idea that we can group together a set of statements into one atomic batch that either fails or doesn't\n* **Consistency** - Very overloaded term in the DB world.  In ACID, it means that certain invariants about data can always be kept true.  Mostly a buzzword since consistency is normally handled by the application and not the database\n* **Isolation** - concurrently executing transactions are isolated from each other.  In its purest sense, it can mean **serializability**, but is usually used to mean a weaker form of isolation.\n* **Durability** - Once a write has been committed, it will not be lost even in the case of hardware failure (impossible to actually promise)\n\n## When Transactions Can Fail\n* Transaction succeeds but network connection to client is down - client will resend transaction\n* Transaction fails due to overloaded DB - we will keep retrying and make the problem worse (might be fixed by exponential backoff)\n* How to retry on a non-transient transaction failure (e.g. a constraint in the DB being violated by insertion)? \n* If the client process fails while retrying, any data it was trying to write to the database is lost\n\n## Concurrency Problems in Databases\n* **Dirty Write** - Two concurrent writes occur.  The first is not committed before the second one is, so the first write is lost forever.\n* **Dirty Read** - A read of an uncommitted a value occurs.  If the transaction needs to be rolled back, this means that the read has seen data that may never make it into the database\n* **Non-repeatable Read or Read Skew** - Two consecutive, identical reads from a DB (without any concurrent writes) result in different outcomes.\n    * Examples of where this would be especially undesirable are in analytical queries or backups, since we want to have a consistent snapshot of the DB at one point in time\n* **Lost Update** - two concurrent updates (both doing Read-Modify-Write) occur on the same object, and one of them is lost\n* **Write Skew**  - a generalization of the lost update problem. Write skew can occur if two transactions read the same objects, and then update some of those objects\n\n## Solutions to Database Concurrency Problems\n* **Read committed**\n    * Dirty writes are prevented using row-level locks (only one write can occur on a row at a time)\n    * Dirty reads are prevented by keeping 2 versions of each row (an uncommitted version and a committed version).  A read always only sees the committed version.\n* **Snapshot Isolation**\n    * Non-repeatable reads can be prevented by using Multi Version Control Concurrency (MVCC)\n      * Many versions of each row are kept and each transaction is assigned an auto-incrementing unique transaction id\n      * Rules for whether a version of a row is visible to a given transaction:\n        * At the time when the reader’s transaction started, the transaction that created the object had already committed.\n        * The object is not marked for deletion, or if it is, the transaction that requested deletion had not yet committed at the time when the reader’s transaction started.\n* **Solutions for Lost Updates**\n    * Atomic updates - Explicit lock is taken on an object on read, which prevents other transactions from reading the object until the update has completed\n    * Explicit locking - locking all rows that can possibly be updated by a single query to ensure that no other writes can get in\n    * Detecting lost updates - Can often be done in conjunction with Snapshot Isolation (becuase all of the data for which version of of a row was read and written with a transaction is maintained by the DB)\n    * Compare-and-set - We can detect whether a write occurred between a read and write within a transaction (Cassandra implements this as a Lightweight Transaction)\n      * UPDATE cycling.cyclist_name SET firstname = ‘Roxane’ WHERE id = 4647f6d3-7bd2-4085-8d6c-1229351b5498 IF firstname = ‘Roxxane’\n    * **Conflict Free Replicated Types**\n       * Diffferent writes all make it into the table, and their results are merged\n       * ex. Counters and Sets (both add and remove versions, although the remov versions have more caveats)"
    }
  ]
}