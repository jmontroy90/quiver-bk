{
  "title": "Chapter 6: Automated Placement",
  "cells": [
    {
      "type": "markdown",
      "data": "The scheduler places pods as best it can on nodes.\n> It does this by considering runtime dependencies, resource requirements, and guiding policies for high availability, by spreading Pods horizontally, and also by colocating Pods nearby for performance and low-latency interactions.\n\nOn a given node, allocatable resources are calculated by taking total resources and subtracting out Kube-reserved and system-reserved (etcd, sshd, all that) resources.\n\nA given Kubernetes scheduler has a set of Predicates / Filters + Policies that define its placement behaviors. Example:\n\n```\n{\n    \"kind\" : \"Policy\",\n    \"apiVersion\" : \"v1\",\n    \"predicates\" : [                       1\n        {\"name\" : \"PodFitsHostPorts\"},\n        {\"name\" : \"PodFitsResources\"},\n        {\"name\" : \"NoDiskConflict\"},\n        {\"name\" : \"NoVolumeZoneConflict\"},\n        {\"name\" : \"MatchNodeSelector\"},\n        {\"name\" : \"HostName\"}\n    ],\n    \"priorities\" : [                       2\n        {\"name\" : \"LeastRequestedPriority\", \"weight\" : 2},\n        {\"name\" : \"BalancedResourceAllocation\", \"weight\" : 1},\n        {\"name\" : \"ServiceSpreadingPriority\", \"weight\" : 2},\n        {\"name\" : \"EqualPriority\", \"weight\" : 1}\n    ]\n}\n```\n\nIf you wanted different Predicates / Policies, you could create a new scheduler with those different ones. Then have your containers scheduled by that schedular by adding some sort of `.spec.schedulerName` to the Pod spec.\n\nWhen a Pod is ready to be scheduled, a set of all nodes is first filtered then ordered per above, and a node is selected from that ordered set. If you want to specify a specific kind of node, you can specify key/value labels under the node selector stanza, e.g.:\n```\n  nodeSelector:\n    disktype: ssd\n```\n\nYou can also use **Node Affinity**, which is yet another tier of filtering like the `nodeSelector` above to guide your Pods towards the best nodes. Here, you have required and optional `Rules`, like:\n\n![IMAGE](quiver-image-url/E5DCC425B5C8F14C94CBEFAAB43690FC.jpg =1198x714)\n\nNode affinity helps you place one Pod on nodes that match criteria, but **Pod Affinity** helps at a broader granularity, since it \"combine(s) rules on domains like node, rack, cloud provider zone, and region\".\n\n![IMAGE](quiver-image-url/88C3849F1F29144BB131892A90F2260F.jpg =1012x662)\n\nOne more helpful component is the **descheduler**, which runs as a separate background service. Here are some policies it supports:\n* `RemoveDuplicates` - \"ensures that only a single Pod associated with a ReplicaSet or Deployment is running on a single node\". This could happen when a node is unhealthy and such new Pods spin up, and THEN the unhealthy node becomes healthy, and you have too many Pods running.\n* `LowNodeUtilization` - \"finds nodes that are underutilized and evicts Pods from other over-utilized nodes, hoping these Pods will be placed on the underutilized nodes\"\n\nSome nodes will not be evicted, like pods with local storage. The descheduler respects all QoS.\n\nFrom least to most flexibility, here's how Kubernetes schedules:\n\n1) `nodeName` - hard-coded, not great\n2) `nodeSelector` - key-value pairs placing a Pod on nodes that match\n3) `Default scheduling alteration` - the default schedular Policies and Predicates are above.\n4) `Pod affinity and antiaffinity` - express dependencies on other Pods (e.g. latency, security) by tethering to higher-granularity groupings like topologies\n5) `Node affinity` - express dependency toward nodes, e.g. hardware, location, etc.\n\n6) `Taints and tolerations` - didn't cover this above, so:\n> Taints and tolerations allow the node to control which Pods should or should not be scheduled on them. For example, to dedicate a node for a group of Pods, or even evict Pods at runtime. Another advantage of Taints and Tolerations is that if you expand the Kubernetes cluster by adding new nodes with new labels, you donâ€™t need to add the new labels on all the Pods, but only on the Pods that should be placed on the new nodes.\n\n7) `Custom scheduler` - BYO!\n\n"
    }
  ]
}