{
  "title": "Reliable, Scalable, and Maintanable Applications",
  "cells": [
    {
      "type": "markdown",
      "data": "Basic idea is that of a data system - encompasses:\n* Storage (databases, HDFS)\n* Caching (Redis, Memcached)\n* Indexing for search (Solr, Elasticsearch)\n* Message queues (Kafka, RabbitMQ, Flink)\n* Computation (Spark)\n* Resource management (Mesos, Yarn, Kubernetes)\n* Your application code!\n\nConcerns include:\n* Scalability\n* Reliability\n* Maintainability\n\n### Reliability\n\nWe want an application that provides reliable performance (even when components are degraded), is resilient to human error and unexpected use cases, and disallows unauthorized access and abuses.\n\nWe want to avoid fault that cause failures - testing models (Netflix's _Chaos Monkey_) that induce localized, well-defined failures can strengthen your resilience to failure-inducing faults.\n\n##### Hardware Faults\nTypical - RAM failures, HD failures, power outages. Traditionally, we used hardware-based resiliency - RAID, dual power supplies, etc. This resiliency was only component-to-component, though. Software-based resiliency is better equipped to handle failures of entire nodes, as is common on cloud platforms.\n\n##### Software Faults\nDefined as faults that can be induced for every instance of an application provided the fault input. Could also be service degradation, or resource exhaustion.\n\n##### Human Error\nTest relentlessly. Design APIs that are flexible, but limit the opportunity for human error (type systems!). Set up QA environments. Surface logging, telemetry, and configs clearly such that iteration and visibility is maximized.\n\n### Scalability\n* How many additional resources are needed to keep performane identical for all users under additional load?\n* Conversely - how is performance affected for all users if resources are kept the same under additional load.\n\nTypically for storage-related benchmarking, _throughput_ is used, but for online services, _response time / latency_ is more instructive.\n\nRemember latency vs. response time:\n* Latency is incurred when a request is waiting to be processed\n* Response time is what a client sees, and includes latency, processing time, and possibly networking delays and such\n\n**Percentiles** are a useful way to get at average user experience. Percentiles like _p50, p99, and p999_ (50th, 99th, and 99.9th percentile) for latency / response time will best capture user experience. Higher percentiles tethered to SLAs need to be aware of high-throughput / demand users, which are often the users with bigger profit margins for your service. \n\n(Side note: percentile approximation algorithms - [forward decay](http://dimacs.rutgers.edu/~graham/pubs/papers/fwddecay.pdf), [t-digest](https://mapr.com/blog/better-anomaly-detection-t-digest-whiteboard-walkthrough/), [hdr-histogram](http://www.hdrhistogram.org/))\n\nRemember for response times - in parallel cases, you're tethered to the slowest processing time of your parallel computation. This effect can be called _tail latency amplification_.\n\nAlso remember the idea of _fan-out_ - basically the multi-valuedness of a join (one-to-many) will hugely affect the cost of servicing a request.\n\n##### Approaches for Coping with Load\nScale-out vs. scale-up is the usual conversation. Remember that an architecture built for 100,000 1Kb requests a second should be very different than an architecture built for three 2Gb requests a minute. Same throughput, vastly different data access patterns.\n\n### Maintainability\nAbstractions are key. Throwaway code can lead to big balls of mud, and operational neglect leads to frustrated engineers spending long hours. A good system needs monitoring and alerting with well-surfaced errors and logs. Documentation is also key.\n\nCan't say it better for data sytems than the book's actual data points:\n\n* Providing visibility into the runtime behavior and internals of the system, with good monitoring\n  *  This involves training too - heap dumps, thread dumps, config and exceptions surfaced\n* Providing good support for automation and integration with standard tools\n  * Avoid explosion of deploy models!\n\n* Avoiding dependency on individual machines (allowing machines to be taken down for maintenance while the system as a whole continues running uninterrupted)\n\n* Providing good documentation and an easy-to-understand operational model (“If I do X, Y will happen”)\n\n* Providing good default behavior, but also giving administrators the freedom to override defaults when needed\n  * This should be apparent in the code with good method calls and class member state, available with easy overrides.\n* Self-healing where appropriate, but also giving administrators manual control over the system state when needed.\n  * So important. Reprocessing mechanisms and data availability is 100% necessary.\n"
    }
  ]
}