{
  "title": "Untitled Note",
  "cells": [
    {
      "type": "markdown",
      "data": "# Structure of a Grodd Config Job\n\nOnce your Gorilla Grodd jobs repository is set up, you can create your job. A Gorilla Grodd config job consists of three components:\n\n1. a **JSON Descriptor** - metadata on your job\n2. a **Spark SQL Query** - the actual transformation\n3. a **JSON-based Schema** - a schema inferred from sample JSONs (more options + schema registry on the roadmap)\n\nEach requirement - the _Descriptor_, _Query_, and _Schema_ are treated as simple Scala Resources. To aid in setting up your job, we have an example job `gg-samplemart-sample-config-projector-single` here. Take a second to familiarize yourself with the structure of this project. A few things to note:\n\nThe main three components - Descriptor, Query, and Schema - are all located on the main branch of your SBT project, under src/main/resources/selfservicejobs/<jobName>/\nThe sample project gives two ways to set up your JSON tests under src/test/resources/selfservicejobs/<jobName>/. More information on tests will be outlined in the \"Testing\" section.\nDescriptor\nThe descriptor.json is a JSON structure that contains metadata about your Jessica Jones job. Some fields are required for every job, while others enable more advanced features. \n\nField\nType\nRequired\nDefault\nDescription\noutput_kafka_topic\nstring\nyes\nn/a\nThe name of the output Kafka topic on the output Kafka cluster\njob_name\nstring\nyes\nn/a\nThe name of your Jessica Jones job - MUST MATCH THE NAME OF YOUR JOB FOLDER!!\ndead_letter_queue_topic\nstring\nyes\nn/a\nThe name of the Kafka topic on the output Kafka cluster to which all job errors \nwill be sent, including failed payloads\nkey_path\nstring\nyes\nn/a\nA JSON path (period-delimited) specifying where to find the value to send as the \nKafka key (each message is a KV pair on Kafka)\nfilter_info\narray<object>\nno\n(no filter applied)\nAn array containing all stream prefiltering information. Consists of one or more \nobjects detailing how to pre-filter. See section called \"Filtering Your Stream\" \nunder the \"Features\" section.\nfilter_info → filterPath\nstring\nno\n\"\"\nA JSON path (period-delimited) to check for existence on each message\nfilter_info → filterVal\nstring\nno\n(value not checked)\nA string value to check against for the specified filterPath\nfilter_info → negate\nboolean\nno\nfalse\nInverts the filter applied to your message stream based on the boolean results \nof the filterPath / filterVal combination\nA sample descriptor.json containing all of the above values might look like the following:\n\n{\n  \"output_kafka_topic\": \"flash-prod-jet-wm-gis-order\",\n  \"job_name\": \"SalesOrder_CustomerOrder_V1\",\n  \"dead_letter_queue_topic\": \"flash-jet-wm-gis-error\",\n  \"key_path\": \"payload.sales_order_key\",\n  \"filter_info\": [\n    {\"filterPath\":\"data.orderAttributes.source_dropship_info.name\", \"filterVal\":\"walmart\", \"negate\":true},\n    {\"filterPath\":\"data.order_id.id\", \"negate\":true}\n  ]\n}\nQuery\nThe query.sql contains a single Spark SQL statement that will be used for processing your job. Spark SQL has a vast library of functionality available for usage, in addition to custom UDFs developed by Mystique. Some starter resources:\n\nSpark SQL Programming Guide\nSpark SQL functions docs\nA sample query might look like the following:\n\nSELECT \n\tnamed_struct(\n\t\t'output_int', cast(data.sample_int as int), \n\t\t'output_key_path', data.sample_key_path \n\t) AS payload \nFROM input_json\n\n\nA few things to note:\n\nSpark SQL can read, transform, and produce nested JSON. This is accomplished through the named_struct function above. named_struct takes two arguments per output column - in other words, you should always have an even number of arguments being passed to it. The first argument per column is the key of the output JSON, while the second argument is the value to populate that key with.\nThe named_struct function requires an alias after the closing parentheses - \"AS payload\" here. This will determine the key of the JSON object that contains all columns within the named_struct call.\nSpark SQL's cast function will change the output JSON data type - here, if data.sample_int was a string, the output will be a JSON integer like \"output_int\": 123.\nThe resulting JSON from this query (with data.sample_int = \"123\" and data.sample_key_path = \"kp\") will look like this:\n\n{\n\t\"payload\": {\n\t\t\"output_int\": 123\n\t\t\"output_key_path\": \"kp\"\n\t}\n}\n\n\nIn addition to built-in Spark SQL functions, Mystique has built out custom User-Defined Functions (UDFs) for particular technical / business challenges. For full documentation on currently available UDFs, please see the docs in the code. For convenience, the list of UDFs and a brief description:\n\nUDF\tDescription\tExample\ngenerate_uuid\tGenerates a unique identifier string\t\nselect generate_uuid() from input\ndate_to_epoch_second\tConverts an input string representing a local timestamp in the given string format to an Epoch second in the given timezone. Handles arbitrary precision, but requires the end user to specify a \"header\" and \"footer\" part of their date format. For example, in timestamp format `yyyy-MM-ddTHH:mm:ss.SSSSSSZ`, the portion `yyyy-MM-ddTHH:mm:ss` would be considered the \"header\" and `Z` would be considered the \"footer\". See the below example for how to escape literals in the timestamp format.\t\nselect date_to_epoch_second(timestamp,\n'yyyy-MM-dd\\'T\\'HH:mm:ss', '\\'Z\\'', 'UTC') from input\nstring_to_guid\tConverts string to ms guid\t\nselect string_to_guid(string) from input\nepoch_millis_to_seconds\tConverts an input Epoch timestamp from milliseconds to seconds\t\nselect epoch_millis_to_seconds(timestamp, 'UTC') from input\n\n\nSchema\nThe inputschema.json contains sample JSONs that are representative of the input JSON that will be transformed by your query.sql. A couple rules of thumb:\n\nDo NOT commit any sample schema with PII. The definition of PII here includes any names, address, emails, phone numbers, or anything else like that.\nEnsure your sample JSON does not contain any null values for fields. This will produce schema with a literal NullType as the expected type of a JSON field, which is almost never the case.\nMake sure your sample JSONs are truly representative - the final schema will be the composite of all schemas inferred from your sample JSONs.\nFor deep dive questions on how schema is inferred and combined, please reach out to Mystique. We've analyzed this stuff closely.\n"
    }
  ]
}