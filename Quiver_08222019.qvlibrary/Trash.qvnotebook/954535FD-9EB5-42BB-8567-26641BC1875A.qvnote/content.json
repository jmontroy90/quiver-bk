{
  "title": "Untitled Note",
  "cells": [
    {
      "type": "markdown",
      "data": "## Grodd Data Flows\n\nAt the highest level, Mystique offers two kinds of data flows to end users. They are:\n\n1) Projectors\n2) Listeners\n\n#### Projectors\n\nA `projector` is a *transforming data flow*. Mystique's basic projector enables transformations\nfrom one data model `A` to another model `B` (`A -> B`). Mystique offers both the entire power of Spark's built-in\ntransformations as well as Jet-specific custom *user-defined functions* (UDFs).\n\n#### Listeners\n\nA `listener` is a *reactive and transforming data flow* - it reacts to projectors! Imagine a projector has just written data into a data\nstore -- a listener job receives a *change stream message* indicating that a table of interest in our data store has\nchanged in some way. With that notification information, a listener job will read from the changed table, and\nthen aggregate the data for the indicated partition + clustering keys as needed.\n\n## Grodd Job Types\n\nHow do our many \"different\" job types relate to one another? We think of them in terms of feature sets, almost like\ntrading cards! For more information on each `Feature` listed, see the next section.\n\n#### 1) Projectors\n\n* _Primary Purpose_: Stateless Transform\n* _Question Answered_: \"How can I perform stateless transformations from Kafka to Cassandra?\"\n* _Input Schema_: yes\n* _Data Inputs_: Kafka\n* _Data Outputs_: Cassandra, Kafka `ChangeStreamMessage`\n* _Features_: `ColumnSelector`, `preProcess`\n\n#### 2) SQL Projector\n\n* _Primary Purpose_: Stateless Transform via Spark SQL\n* _Question Answered_: \"Can I do a Projector using Spark SQL in my job?\"\n* _Input Schema_: yes\n* _Data Inputs_: Kafka\n* _Data Outputs_: Cassandra, Kafka `ChangeStreamMessage`\n* _Features_: `Spark SQL`, `preProcess`\n\n#### 3) Listeners\n\n* _Primary Purpose_: Aggregations\n* _Question Answered_: \"I want to aggregate the results of a projector.\"\n* _Input Schema_: no (`ChangeStreamMessage`)\n* _Data Inputs_: Kafka `ChangeStreamMessage`, Cassandra\n* _Data Outputs_: Cassandra, Kafka `ChangeStreamMessage`\n* _Features_: `UDF`\n\n#### 4) Source Listeners (\"stateful projector\")\n\n* _Primary Purpose_: Stateful Transform\n* _Question Answered_: \"I want a projector, but with state from Cassandra to aid in my transform.\"\n* _Input Schema_: yes\n* _Data Inputs_: Kafka, Cassandra (supplemental)\n* _Data Outputs_: Cassandra, Kafka `ChangeStreamMessage`\n* _Features_: `ColumnSelector`, `preProcess`, `UDF`, `read-modify-write`\n\n*Note*: this job type is _poorly named_ - it's really just a stateful projector.\n\n#### 5) Join Mart Listeners\n\n* _Primary Purpose_: Stateful Transform\n* _Question Answered_: \"I want a listener, but with the ability to join two tables.\"\n* _Input Schema_: no (`ChangeStreamMessage`)\n* _Data Inputs_: Kafka `ChangeStreamMessage`, Cassandra (joining / supplemental)\n* _Data Outputs_: Cassandra, Kafka `ChangeStreamMessage`\n* _Features_: `UDF`, `JoinKey`\n\n#### 6) SQL Join Mart Listeners\n\n* _Primary Purpose_: Stateful Transform\n* _Question Answered_: \"I want a join mart listener, but with the ability to query my input table with Spark SQL.\"\n* _Input Schema_: no (`ChangeStreamMessage`)\n* _Data Inputs_: Kafka `ChangeStreamMessage`, Cassandra  (joining / supplemental)\n* _Data Outputs_: Cassandra, Kafka `ChangeStreamMessage`\n* _Features_: `UDF`, `JoinKey`, `Spark SQL`\n\n*Note*: this is SQL querying within an application, not from a resource file.\n\n#### 7) Pass-through Mart Listener\n\n* _Primary Purpose_: Data Movement\n* _Question Answered_: \"I just wrote my data to Cassandra via a Projector - can I get that written data into Kafka?\"\n* _Input Schema_: no (`ChangeStreamMessage`)\n* _Data Inputs_: Kafka `ChangeStreamMessage`, Cassandra\n* _Data Outputs_: Kafka\n* _Features_: `Pass-through`\n\n## Features\n\n* `ColumnSelector` — a Mystique API for easily transforming an input Dataset, using both built-in and custom transforms\n* `preProcess` - the ability to prefilter / pretransform your input Dataset; usually used on heterogenous Kafka topics\n* `Spark SQL` — the ability to query your input Dataset using Spark SQL within your application (NOT within a resource)\n* `UDF` — a generic interface for performing a transform (`T` in `ETL`); right now used as the transform mechanism for all listeners for transforming from `InputType` -> `OutputType`.\n* `read-modify-write` — the ability of a job to read, change and then write records without fear of concurrency issues / race conditions. This is currently achieved via a shuffle on our input data.\n* `JoinKey` — the ability of a job to join two Cassandra tables via various logical joins (\"full\", \"left\", etc.)\n* `Pass-through` - the ability of a job to write some data from Cassandra back to a Kafka topic"
    }
  ]
}