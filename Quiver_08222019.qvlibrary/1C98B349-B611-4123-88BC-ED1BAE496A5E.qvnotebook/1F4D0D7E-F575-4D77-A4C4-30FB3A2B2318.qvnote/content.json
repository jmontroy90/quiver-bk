{
  "title": "Chapter 12. Performance Tuning",
  "cells": [
    {
      "type": "markdown",
      "data": "## Overview\nFirst thing - performance goals should revolve around:\n* Throughput - queries completed / second\n* Latency - time to complete one query\n\n`nodetool` is your friend! Examples:\n* `nodetool proxyhistograms` - read / write / range latency of queries for which the given node has served as the coordinator.\n* `nodetool tablehistograms` - same sorts of stats for an individual table, including SSTable data\n\n##### Analyzing Performance Issues\nQuery-first design is crucial - partitions that are too large result too much data fetched, read repair failing, and so on. Small partitions lead to massive partition lookup for simple queries. Both bad.\n\n##### Tracing\nSet `TRACING ON` in cqlsh and you get a LOT of good info. Watch out for multi-node queries! Anything with secondary indexes are probably not your friend.\n\nThe DataStax Java driver also exposes query trace information on the `SimpleStatement` that's executed by a `Session`.\n\nTraces are stored in a system table, and are TTLed per a system configuration.\n\nNow let's look at some ways we can actually tune (beyond just adding resources!).\n\n## Caching\nRemember - all these caching settings are DDL, so they exist at the table level. You can't ask for caching as a client!\n\nKey caches are enabled by default to facilitate key lookups to row index entries, which helps because keys are usually small and indexes are good!\n\nRow caches are good for frequently-accessed data, but obviously row cache bloat with a bad hit rate is nothing but bad. You can specify how many rows \n\nSample cqlsh for this stuff:\n```\nALTER TABLE hotels\nWITH caching = { 'keys' : 'ALL', 'rows_per_partition' : '200' };\n```\n\nYou also have a counter cache for frequently-accessed counters, used to reduce lock contention. Meh. It's 2.5% of the JVM heap or 50MB, whichever comes first.\n\nLast - caches are saved to disk for reboot scenarios, and caches can be invalidated. The latter is a classic move for benchmarking, right?\n\n## Memtables\nYou can configure:\n* How much of your memtable are stored on- vs off-heap.\n* How many writer threads are used to flush a memtable.\n* Which implementation manages memory in Cassandra, on- and off-heap (NIO by default).\n* Metered flushing - flush on an interval before commit log / memtable max size hit.\n\n## Commit Log\nAll the same stuff here. How do you roll the commit log size-wise, how often are logs removed, are they compressed, and so on. \n\nYou can do `batch` vs `periodic` syncing from the OS buffer cache to disk. `periodic` is the default - performance will improve, but you risk losing data! `batch` guarantees durability. Remember - this \"commit log buffer\" has nothing to do with Cassandra, and is just an `fsync` from the OS buffer cache.\n\n## SSTables\nCommit logs are (mostly) synchronous - SSTables are not. If you've got hard disks, use separate disks for commit logs and SSTables. For SSDs, one disk is fine.\n\nSSTables are loaded into off-heap memory into buffer caches to help with read access patterns. Things like `buffer_pool_use_heap_if_exhausted` will let you spill into on-heap if needed.\n\n## Hinted Handoff\nYou can actually control the bandwidth utilization of hint delivery! Other things include the size of hints directory and the TTL of hints.\n\n## Compaction\nThree main compaction strategies, good to know!\n* `SizeTieredCompactionStrategy` - SSTables can be any size. This strategy orders and groups them into tiers (basically a \"minSize2 <= SSTable < minSize3\"), and when there are `N` SSTables in that tier, they're compacted. This is a good strategy for lots of writes, but reads, less so - data might be sloshed around many SSTables!\n* `LeveledCompactionStrategy` - if your reads really outweigh your writes, this strategy allows you to organize same-sized SSTables (default 5mb) into levels, where each level has 10x the number of SSTables as the previous level. Moreover, each partition key is guaranteed to only exist in one SSTable per level! Really good for read speed, but Cassandra will choke if you've got a ton of writes as well.\n* `DateTieredCompactionStrategy` - a newer strategy. Compacts SSTables by their write time, so very good for optimizing reads on recent data! Read carefully before using.\n\nYou can customize the number of SSTables in the compaction queue needed to kick off an actual minor compaction. \n\n## Networking and Timeouts\nSummed up best by:\n![IMAGE](quiver-image-url/363641956DBBD20A7EDD9CD462AADFA6.jpg =1300x1108)\n\n## JVM Settings\nCassandra nodes will use 25% of system memory for nodes with >4G memory (50% for less). Play with min / max heap size first when you tune!\n\nRemember - every Java object has an `age` field in its header!\n\nG1GC is still the general purpose winner. IT divides your heap into logical segments for Eden, survivor, and old generation segments. This enables the algorithm to require fewer stop-the-world pauses like CMS does. It wasn't the default in 2.0 because for machines with less RAM, CMS still wins.\n"
    }
  ]
}