{
  "title": "7/2 - 7/9",
  "cells": [
    {
      "type": "markdown",
      "data": "**7/4**\n\nDid a lot of Splunk sleuthing. Need to check out the Guardians index for an increase in errors related to these sorts of things around 7/1 and 7/2:\n\n```\ninvalid character '<' looking for beginning of value \nfailed to initialize task \\\"phaser-echo-server-task\\\"\nerror OOM Killed\n(rpc error making call: EOF)\norphaned allocations\nError refreshing service (Client.Timeout exceeded while awaiting headers)\nunable to fetch job info: Unexpected response code: 404 (job not found)\n```\n\nDevOps didn't indicate any broader problems, but I'm skeptical.\n\n**7/2**\n\nDAY 1 CONCLUSIONS:\n1) Redeploy for Nomad upgrade - roll together with client upgrade and email, can we do Tuesday? (7/9)\n2) Redeploy for JAR SAS token expiration - need list from Justen / DevOps!\n3) Nomad instability is crazy - send email to HashiCorp and revisit some older failures to do some sleuthing.\n4) Kafka random failures on random topic partitions - is this losing us records? Ask in #kafka channel.\n\n---\n\n* spark-nomad has weird reallocating behavior. gotta ping hashicorp. check the nomad channel for more, and let me know if i’m missing something there.\n* i’m gonna move mystique-job-registry configs into mystique-jobs. done.\n* we have confusing versioning on mystique-jobs jars. basically, you want the ones that are like `0.4, 0.5, 0.6-SNAPSHOT` as the `version.sbt` currently states. the ones like `1.118` are left over from our attempt to merge jobs into mystique-parent. DON’T use those.\n* how the heck did we end up missing a `m` for megabytes in our executor memory config? mystery of our times… (edited)\n* there's some serious nomad wonkiness going on today. justen indicated re-allocating problems, but i'm seeing lost drivers, jobs not accepting resources, jobs that never have issues dying due to network issues...so on.\n* did we move from `p2nmdlspk` to `p2nmdlsps` or something?\n* getting a lot of this for one or two jobs. I've seen this on other jobs too. Looking at the job, there's literally just one task / partition lagging behind on a Kafka poll, and it seems to be totally random which Kafka partition times out.\n```\n\"java.lang.AssertionError: assertion failed: Failed to get records for spark-executor-rao-checkedout-listener mystique-gorilla-grodd-change-stream-rsku-checked-out-p8 2 878055 after polling for 4096\"\n```\n8) holy heck lots of pages\n9) redeploy for JAR SAS tokens expiring?\n10) redeploy for Nomad incompatibility issue?\n```\nTLDR: Redeploy Nomad Spark Jobs.\n \nWe’re upgrading nomad in production on Monday next week (July 09, 2018). However the new version of nomad comes with a breaking change that stops converting “.” In environment variables to “_”.\nThis affects the spark submitter and it has already been updated to take care of this. However, existing spark jobs will need to be redeployed so they do not fail when they are rescheduled after the upgrade.\n```\n\n\nrui's job does a TON of lookups. no wonder RUs are so high.\n\nconclusion on store-price / congo / all of rui's stuff: superman discovered a data source problem with their topic, and thus all the data needs to be nuked and they need to do some work to get onto the new data source. as such, we've left their jobs on but disabled all alerts"
    }
  ]
}