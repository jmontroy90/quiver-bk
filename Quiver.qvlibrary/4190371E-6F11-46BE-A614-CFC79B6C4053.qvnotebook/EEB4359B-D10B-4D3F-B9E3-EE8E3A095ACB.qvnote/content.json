{
  "title": "High-Performance Spark",
  "cells": [
    {
      "type": "markdown",
      "data": "#### Off-heap Storage\nTungsten allows your application to store partition data off-heap - this allows you to avoid typical GC overheads and work with smaller objects because they don't have to conform to JVM object requirements.\n\n#### RDD Wide Dependencies and Fault-Tolerance\n\nBecause a child RDD might have many parents in wide transformations, a task / partition failure requiring a recompute of the DAG is much more expensive than a failure for narrow transformations. Checkpointing (saving intermediate results) can mitigate this (as could a successful `persist`, I suppose).\n\n#### RDD Coalesce\n\nRemember that a `coalesce` can reduce the number of partitions - this would be a _narrow_ transformation. Increasing partitions with `coalesce` is a _wide_ transformation.\n\n#### Array Storage for Memory Efficiency\nScala Arrays are the most memory-efficient collection in Scala, and if you are capturing data in a case class or tuple, you might consider using an array with well-defined index points to store data instead!\n\n#### Implicit Conversions => Object Creation\nBe careful with implicit conversions - they can litter unexpected objects in your JVM. Think of things like `Traversable` to `TraversableOnce` and the like.\n\n#### Iterator-to-Iterator in mapPartitions\nAn iterator is essentially lazy, and should be viewed as a traverse-once set of instructions. As such, reifying the iterator into a bona fide collection is incurring possibly uneccessary overhead. Try to make your `mapPartitions` body free of collection conversions by sticking to transformations (`map`, `filter`), things that don't require full iterator traversals all at once.\n\n#### Shuffle Files + Skipping Stages\nShuffle files are written to disk during a shuffle, containing all input records for the shuffle. In case of an RDD recompute, these shuffle files can be used to skip shuffle stages, simply using the shuffle files to rebuild the RDD. This can lead to out-of-disk errors for long-running jobs!\n\n#### Job Scheduling\nSpark is FIFO for jobs, but has an option for Fair Scheduling. Be aware in case you have some jobs hogging your cluster. Fair Scheduling also allows for priority queues.\n\n"
    }
  ]
}