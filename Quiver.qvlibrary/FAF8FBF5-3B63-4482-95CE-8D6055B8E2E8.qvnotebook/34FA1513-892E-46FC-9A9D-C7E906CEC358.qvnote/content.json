{
  "title": "Chapter 20 - Load Balancing in the Datacenter",
  "cells": [
    {
      "type": "markdown",
      "data": "The goal of load balancing is to spread load as equally as possible across backends. We have a set of backends, and one of those backends will be chosen for a given client. The client's connection will be maintained, s.t. individual requests can be sent without reinitiating connection setup. We can configure rate limits on backends to control how many active requests it can handle simultaneously, and then mark it as unhealthy until that state subsides. It gossips its state (healthy, unhealthy) to LBs and other backends.\n\nThey discuss the **lame duck state** -- a prep step for a clean shutdown. If you just kill backend, all in-flight requests will be hard-killed and have to retry (or worse, data loss?)? Instead, issue a SIGTERM first by marking the backend as \"lame duck\" -- it will accept no more requests, while finishing out in-flight ones. Once enough time has elapsed, the backend is actually killed (so this is a configurable heuristic -- how long should it take to finish processing in-flight requests? SRE book guesses between 10 - 150 seconds).\n\n**Subsetting** is a technique to evenly distribute load -- basically, choose some subset of backends for clients to interact with, instead of the whole pool every time. They found that random subsetting lead to uneven load, but deterministic subsetting via \"rounds\" worked well. Shrug, main thing of import is that you should choose a subset of backends per client.\n\nRemember that clients can incur uneven load on backends too, which should be factored in.\n\n### Load Balancing Policies\nOnce we've get our datacenter (see previous chapter) and our backend subset (above), let's actually select a backend. Three methods discussed:\n1) **Round Robin** -- each backend takes its turn, then repeat. Can have problems with:\n    * uneven client load -- \"query one id\" vs. \"query emails from last three months\"\n    * diversity of machines -- some machines have less beefy CPUs (Google solved by moving to Google Computer Units, which allows for conversion of resource requests across CPUs)\n    * unpredictable task performance -- initialization of JVMs (steady-state), task restarts, noisy neighbors, unexpected GC, etc.\n2) **Least-Loaded Round Robin** -- each client task will send to the backend task with the fewest active requests. Pitfalls:\n    * _Sinkholing_ -- a failed task returns super speedy \"i'm unhealthy\" responses, meaning it always has few requests and keeps recieving traffic. Fix by treating errors as active requests, so the server will quickly show overloaded.\n    * Clients make local decisions -- they don't know how many requests other clients have sent to shared backends, only their own request counts.\n    * Request count is crude -- some machine might be able to handle twice as many requests, but because its network latency is comparable to some other machine's, they'll get similar numbers of requests even though they're only the same because of the network, NOT the machine.\n3) **Weighted Round Robin**:\n    > Weighted Round Robin is fairly simple in principle: each client task keeps a \"capability\" score for each backend in its subset. Requests are distributed in Round-Robin fashion, but clients weigh the distributions of requests to backends proportionally. In each response (including responses to health checks), backends include the current observed rates of queries and errors per second, in addition to the utilization (typically, CPU usage). Clients adjust the capability scores periodically to pick backend tasks based upon their current number of successful requests handled and at what utilization cost; failed requests result in a penalty that affects future decisions."
    }
  ]
}