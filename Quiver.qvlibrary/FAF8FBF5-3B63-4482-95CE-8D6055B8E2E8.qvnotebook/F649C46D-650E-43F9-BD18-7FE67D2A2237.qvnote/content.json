{
  "title": "Chapter 21 - Handling Overload",
  "cells": [
    {
      "type": "markdown",
      "data": "# Handling Overload\n\n> At the end of the day, it's best to build clients and backends to handle resource restrictions gracefully: redirect when possible, serve degraded results when necessary, and handle resource errors transparently when all else fails.\n\n### The Pitfalls of \"Queries per Second\"\nMuch like IOPS, QPS is meaningless without context. Different queries have different access patterns and costs. Better to understand your query costs in terms of the resources it consumes (which you can control up front, or discover through data) -- how much CPU, how much memory, with these quantities normalized across different architectures (faster / slower CPU speeds, Google's notion of \"Compute Units\"). If you CAN'T control your query cost at the client side, something is generally wrong.\n\n### Per-Customer Limits\n\nNegotiate your expected resource utilization, e.g. your quotas, up front with each client. That way, known client integrations that start to misbehave will not impact other integrations, and unknown ones can be limited to some highly-throttled starter rate, or blocked entirely.\n\n> An interesting part of the puzzle is computing in real time the amount of resources—specifically CPU—consumed by each individual request. This computation is particularly tricky for servers that don't implement a thread-per-request model, where a pool of threads just executes different parts of all requests as they come in, using nonblocking APIs.\n\n### Client-Side Throttling\nAny client that hits their quota limit will recieve some error indicating as such, but they could ignore that and still spam requests, keep the backend overloaded to some degree. Client-side throttling helps with that -- when you hit your quota limit, the client will use **adaptive throttling** and calculate the following probability that a request will be rejected _client-side_:\n\n![IMAGE](quiver-image-url/1F97BA4BCD71E272FDF8E425D4F945E2.jpg =295x51)\n\nThis `K` determines how aggressively the client throttles. Google chooses `2` because:\n\n> By allowing more requests to reach the backend than are expected to actually be allowed, we waste more resources at the backend, but we also speed up the propagation of state from the backend to the clients. For example, if the backend decides to stop rejecting traffic from the client tasks, the delay until all client tasks have detected this change in state is shorter.\n\nThis makes sense -- recency of information is important for determining the best course of action.\n\n## Criticality\n\nEmbed some priority into yoru requests, so we can intelligently choose what traffic gets served and what gets shed in an overload scenario:\n\n1. **`CRITICAL_PLUS`** - Reserved for the most critical requests, those that will result in serious user-visible impact if they fail.\n2. **`CRITICAL`** - The default value for requests sent from production jobs. These requests will result in user-visible impact, but the impact may be less severe than those of CRITICAL_PLUS. Services are expected to provision enough capacity for all expected CRITICAL and CRITICAL_PLUS traffic.\n3. **`SHEDDABLE_PLUS`** - Traffic for which partial unavailability is expected. This is the default for batch jobs, which can retry requests minutes or even hours later.\n4. **`SHEDDABLE`** - Traffic for which frequent partial unavailability and occasional full unavailability is expected.\n\nThese criticalities are propagated through services, and can be configured with individual limits. Very useful!\n\n## Utilization\n\nHow does a backend task actually indicate its utilization, e.g. signal that quota is exceeded? You have to look at query patterns, how they fan out into threads / goroutines, and how long they live. Usually you can just do used CPU / allocated CPU, but you have to smooth out your utilization signal to account for massive bursts of short-lived activity.\n\n## Handling Overload Errors\n\nSome overload errors indicate \"a large subset of backend tasks are overloaded\", which means DON'T retry, error to the user. If \"a small subset of backend tasks are overloaded\", that means something's up with the datacenter's in-cluster load balancing, and you can just retry and hope to hit a better backend / subset. These retries are indistinguishable from normal requests, which is good.\n\n## Deciding to Retry\n\nTwo \"budgets\" of retries:\n* per-request retry budget\n* per-client retry budget\n\nPer-request can be handled by the client, but better for it to be handled by backends -- if you have a chain of calls, you want it to retry only where it failed, and can return \"overloaded; don't retry\" to backends closer up the stack to the client. Per-client just guarantees a globally-aware way of not going overboard with individual requests and their retries.\n\nBe careful of combinatorial explosions of retries! Understand the call stack and how a retry at each layer is propagated both up and down that stack.\n\n## Load from Connections\n\nMaintaining connections takes resources. Even if you downgrade a connection from TCP to UDP and periodically healthcheck, you can still end up in trouble.\n\nSpinning up a lot of connections at once (batch jobs) is hard too -- ameliorate by spreading load across DCs, or putting a \"batch proxy\" between the client and its backends. A batch proxy would just handle request forwarding in a healthy, throttled way.\n\n## Conclusion\n\nA lot of these techniques rely on propagating state in distributed systems, and using that state to make intelligent decisions. Make sure that, in light of a catastrophic failure, your services can still service traffic intelligently without that global distributed state.\n\n\n"
    }
  ]
}