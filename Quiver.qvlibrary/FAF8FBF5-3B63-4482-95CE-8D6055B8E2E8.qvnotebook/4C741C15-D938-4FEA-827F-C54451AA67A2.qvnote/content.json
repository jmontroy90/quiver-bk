{
  "title": "Chapter 12 — Effective Troubleshooting",
  "cells": [
    {
      "type": "markdown",
      "data": "Main points to make troubleshooting easier:\n1) Building observability — with both white-box metrics and structured logs—into each component from the ground up\n2) Designing systems with well-understood and observable interfaces between components\n\nStop the bleeding:\n\n> Your first response in a major outage may be to start troubleshooting and try to find a root cause as quickly as possible. Ignore that instinct! Instead, your course of action should be to make the system work as well as it can under the circumstances. This may entail emergency options, such as diverting traffic from a broken cluster to others that are still working, dropping traffic wholesale to prevent a cascading failure, or disabling subsystems to lighten the load.\n\nThis might mean cordoning / draining a problem node in k8s, killing a problem pod, etc.! It's very tempting to try to figure out what's going wrong and root-cause, but your first priority is to stop the bleeding first, not figure out the exact cause. You **should**, however, try to preserve logs and other relevant state as you triage the issue.\n\nMake sure you understand the problem, then hypothesize -- test your hypothesis by looking at metrics / logs, and seeking confirming and disconfirming evidence of your hypothesis. If you can't state **at the outset** what the basic problem is and what might be causing it, you need to step back and / or increase your knowledge of the system.\n\nSources to look at:\n1) Metrics (Prometheus)\n2) Logs (Splunk, structured, leveled)\n3) Current system state (config, logging levels, sample requests / responses) -- really useful, Envoy does this well!\n4) Instrumentation -- can you add a debugger (Go does this well) or hit endpoints directly to see the results?\n\nInvestigate by looking at system boundaries -- component by component, what's the expected input and output? This is just **dividing and conquering** -- work through the stack front to back, ensuring expectations at each step. If this takes too long, you can **bisect** -- pick a halfway point and make sure _those_ inputs / outputs work, e.g. eliminate half of your systems like a binary search. IF you're dealing with high latencies on a service, start with the frontend server, then the load balancer, then get the list of backends, hit those directly, hit their database(s), and so on.\n\nMake sure you ask -- **what** is this doing, and what should it be doing? **Why** is it doing the wrong thing? And **where** is that wrong thing happening?\n\n**Recent changes** are correlated with problems -- did a deploy happen? If it's hard to track down recent system changes, you'll have a problem correlating root cause. Most issues I see are introduced by humans, machines are more reliable.\n\nTesting a theory can be hard:\n1) Make your tests mutually exclusive.\n2) Start with most likely theory, then move to less likely.\n3) Things like network connectivity tests can be confounded by firewalls, whitelisting, RBAC, TLS, etc.\n4) Testing itself can confound results -- increased logging increases CPU, increased CPU increases race conditions / deadlocks, etc.\n\n**Negative results** are useful -- they can quickly rule out courses of action.\n> Do your part by telling everyone about the designs, algorithms, and team workflows you’ve ruled out. Encourage your peers by recognizing that negative results are part of thoughtful risk taking and that every well-designed experiment has merit. Be skeptical of any design document, performance review, or essay that doesn’t mention failure. Such a document is potentially either too heavily filtered, or the author was not rigorous in his or her methods."
    }
  ]
}