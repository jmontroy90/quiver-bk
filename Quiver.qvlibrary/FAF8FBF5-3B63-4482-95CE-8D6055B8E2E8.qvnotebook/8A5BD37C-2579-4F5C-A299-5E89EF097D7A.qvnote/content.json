{
  "title": "Chapter 17 - Testing for Reliability",
  "cells": [
    {
      "type": "markdown",
      "data": "In order to maintain and code for reliability, we need tests. Tests let us confidently describe all changes made to a system by verifying important behaviors before the software is deployed.\n\nSome metrics we should track around reliability testing:\n* **Mean Time To Repair** (MTTR) -- either through a rollback or a bug fix, how long does it take to fix a bug in a deployed system?\n    * A MTTR of zero mean you caught a bug in your testing, whether it be unit, integration, or end-to-end tests. This is what you want.\n* **Mean Time Between Failures** (MTBF) -- the longer users can go without experiencing a failure in your system, the better.\n\n## Types of Software Testing\n\nTwo kinds of tests:\n1) **Traditional tests** -- offline during development; unit, integration, system. Tests software correctness.\n    1) _Unit tests_: single unit (class, function); good for TDD\n    2) _Integration tests_: still usually per code base; mocks come in to specify exist behaviors for components (e.g. database, web service).\n    3) _System tests_: large-scale tests, multiple components / modules. Comes in multiple flavors:\n        * **Smoke tests**: basic, crucial functionality; sanity tests; serve to short-circuit the need to run more complex tests.\n        * **Performance tests**: tests that, over time, a system will not degrade in performance (usually invisibly)\n        * **Regression tests**: a gallery of bugs that have previously screwed up your system, so that we can prevent sneaky bugs from re-entering your codebase.\n2) **Production tests** -- online tests; tests that software is working correctly.\n\nRollouts entangle tests. Rollouts are done in stages and usually don't correspond to one version of code, config, and other dependencies.\n> For example, the test might use the latest version of a configuration file located in source control along with an older version of the binary that’s live. Or it might test an older version of the configuration file and find a bug that’s been fixed in a newer version of the file.\n\nTesting config is hard. Diffing new config to deployed config can show the distance of your changes from prod, and you can test config changes to ensure they'll take. Defaults, hard-coding, and other sneaky side effects on your config make things hard. Probably it's best to exactly specify your full config at deploy time, and also make it viewable (just an endpoint). Maybe save your config, and if it's alterable at runtime (probs not great, maybe just do logging is alterable), make sure THOSE changes are captured.\n\n**Stress tests** push a system to its limits -- remember, most systems don't degrade gracefully, they catastrophically fail.\n\n**Canary tests** are crucial to doing _structured user acceptance_. If we can \"bake the binary\" successfully on some subset of install locations / subset of users, we can proceed with the rest of the rollout.\n\n## Creating a Test and Build Environment\n\nWhen joining a team, start adding tests by \"most impact, least effort\":\n1) Rank your system's components by importance.\n2) Are there mission-critical components, e.g. billing?\n3) Which components have integrations with other teams?\n\n> One way to establish a strong testing culture is to start documenting all reported bugs as test cases. If every bug is converted into a test, each test is supposed to initially fail because the bug hasn’t yet been fixed. As engineers fix the bugs, the software passes testing and you’re on the road to developing a comprehensive regression test suite.\n\nFocus on integrating your tests into your CICD pipeline, and have people react to breaks of those tests quickly. You don't want everyone working around the same bug.\n\n## Testing at Scale\n\nThis was a really scattered section. Or maybe I'm just sleepy.\n"
    }
  ]
}