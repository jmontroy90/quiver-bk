{
  "title": "Chapter 6 - Monitoring Distributed Systems",
  "cells": [
    {
      "type": "markdown",
      "data": "Big takeaways:\n* PagerDuty fatigue is real. Any good monitoring and alerting system must have high signal and low noise.\n* SRE teams should carefully avoid any situation that requires someone to stare at a screen to watch for problems.\n* Avoid complex dependency hierarchies, e.g. \"if X happens, alert on this and scale this, but only if Y is also high\". Systems change, bugs are introduced with complexity. Keep your threshold- / causality-based monitoring rules as simple as possible, or as long-term as possible so that extreme outliers won't make your system go crazy.\n\nPurposes of monitoring:\n* analyzing long-term trends (database growth, user growth)\n* comparing over time or experiment groups (new versions, AB testing, week-over-week performance)\n* alerting (PD, high-priority and low-priority)\n* building dashboards (four golden signals)\n* conducting ad hoc retrospective analysis (debugging, investigating, splunk sleuthing)\n\n**Black-box monitoring**: seeing the system from a user perspective; good for paging and things happening right now.\n**White-box monitoring**: internals of a system (memory, latencies, requests/second); good for debugging.\n\n**Four golden signals**:\n1) **Latency** -- be sure to distinguish latencies between request cases, e.g. successes vs. errors. They might have radically different latencies.\n2) **Traffic** -- requests/second on an HTTP service, transactions/second or retrievals/second for databases, etc.\n3) **Errors** -- distinguish between actual errors (500s) vs. implicit errors (200s with unexpected results).\n4) **Saturation** -- how \"full\" your system is: disk, CPU, memory; performance usually degrades before 100% saturation. Tail latencies can help with this.\n\nBucket your signals to capture tail latencies -- **don't use averages.** Example of exponential buckets:\n> How many requests did I serve that took between 0 ms and 10 ms, between 10 ms and 30 ms, between 30 ms and 100 ms, between 100 ms and 300 ms, and so on?\n\nMake sure your monitoring is at the right granularity. You don't want to miss CPU spikes every three seconds, but you don't need to check for a full disk every two minutes.\n\nGuidelines to manage complexity:\n* The rules that catch real incidents most often should be as simple, predictable, and reliable as possible.\n* Data collection, aggregation, and alerting configuration that is rarely exercised (e.g., less than once a quarter for some SRE teams) should be up for removal.\n* Signals that are collected, but not exposed in any prebaked dashboard nor used by any alert, are candidates for removal.\n\nGuidelines on discussing new paging rules:\n* Does this rule detect an otherwise undetected condition that is urgent, actionable, and actively or imminently user-visible?\n* Will I ever be able to ignore this alert, knowing it’s benign? When and why will I be able to ignore this alert, and how can I avoid this scenario?\n* Does this alert definitely indicate that users are being negatively affected? Are there detectable cases in which users aren’t being negatively impacted, such as drained traffic or test deployments, that should be filtered out?\n* Can I take action in response to this alert? Is that action urgent, or could it wait until morning? Could the action be safely automated? Will that action be a long-term fix, or just a short-term workaround?\n* Are other people getting paged for this issue, therefore rendering at least one of the pages unnecessary?\n\nPhilosophy rules:\n* Every time the pager goes off, I should be able to react with a sense of urgency. I can only react with a sense of urgency a few times a day before I become fatigued.\n* Every page should be actionable.\n* Every page response should require intelligence. If a page merely merits a robotic response, it shouldn’t be a page.\n* Pages should be about a novel problem or an event that hasn’t been seen before."
    }
  ]
}