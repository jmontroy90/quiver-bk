{
  "title": "Spanner: Google’s Globally-Distributed Database",
  "cells": [
    {
      "type": "markdown",
      "data": "[Spanner: Google’s Globally-Distributed Database](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/39966.pdf)\n\nSpanner is \"a database that shards data across many sets of Paxos state machines in datacenters spread all over the world\". Replication is for locality and safety, fail-over is automatic. Spanner focuses on strong consistency, and a semi-relational model beyond Dynamo's KV model. Another good quote:\n\n>Data is stored in schematized semi-relational tables; data is versioned, and each version is automatically timestamped with its commit time; old versions of data are subject to configurable garbage-collection policies; and applications can read data at old timestamps. Spanner supports general-purpose transactions, and provides a SQL-based query language\n\nIt also apparently provides consistency in distributed systems:\n\n> Spanner has two features that are difficult to implement in a distributed database: it provides externally consistent [16] reads and writes, and globally-consistent reads across the database at a timestamp. These features enable Spanner to support consistent backups, consistent MapReduce executions [12], and atomic schema updates, all at global scale, and even in the presence of ongoing transactions\n\nDope! How does it do it?! With globally-significant timestamps that help enforce linearizability (total broadcast order), as well as intelligently-implemented Paxos for 2PC. The \"key enabler\", though, is the new `TrueTime` API. I think. That's the guess. Let's find out.\n\n#### Implementation\n\nSpanner's organizational structure:\n\n![IMAGE](quiver-image-url/922F692D33E6A5774C97673128984098.jpg =412x295)\n\n* One Spanner deploy is a `universe`.\n* `Zones` are an administrative bundle, and can be deployed across datacenters\n* A zone's `zonemaster` handles data placement on spanservers, which each zone has many of.\n* A `spanserver` actually serves data to clients.\n* A zone's `location proxy` handles routing of data requests to a particular spanserver that owns that data.\n\n##### Spanservers in Detail\n\nEach spanserver owns 100 - 1000 instances of a `tablet` data structure (like BigTable) — each tablet is simply a bag of:\n> (key: String, timestamp: int64) -> string\n\nAll tablet data is stored via B-Tree-like structures on disk, as well as a write-ahead log; these both sit on a DFS called Colossus (successor to GFS).\n\nHere's where Paxos state machines enter — **we use them to ensure consistent replication between tablets!** Each tablet has a Paxos state machine with long-lived leaders and time-based leader leases (10 seconds). Writes will initiate a Paxos protocol at a leader, which leads a set of replicas called the Paxos group. If a transaction involves only one Paxos group, it's sufficient to have a tablet and a corresponding lock table; if it involves multiple Paxos groups, then we need a `transaction manager`, which coordinates with the participant leaders (now slaves) of the other Paxos groups.\n\nThis is a lot of nomenclature with a lot of theory behind it — diagram is easier:\n![IMAGE](quiver-image-url/FA1CDC96BEF529C0064756A5CF4A669C.jpg =311x286)\n\n##### Directories\n\nA `directory` is the Spanner unit of data movement, and allows for the placement of contiguous keys with other commonly-accessed data. A single tablet will contain many directories that can span multiple zones with a given Paxos group. Not gonna focus on this, although it's definitely interesting for data access patterns and colocation. There's also a `movedir` background process to move around directories between Paxos groups - presumably to reduce load or gie better access patterns for commonly co-accessed data. Another day I'll look at configuration and setup code for running spanservers / tablets, and it'll help clarify how keys get partitioned and colocated within this architecture.\n\n#### Data Model\n\nThe Spanner data model is \"based on schematized semi-relational tables, a query language, and general-purpose transactions\". The success of Google's Megastore and Dremel show the hunger for semi-relational tables with a query language, respectively. Meanwhile, Spanner just chose to take the performance hit of transactions via two-phase commit, because they think it's better to have app developers code around performance lag rather than code around a lack of transactions / consistency.\n\nThe query language is fairly standard; there are databases and tables. Multiple primary keys expose still KV-esque operations (\"PK columns -> non-PK columns\"). There's also an `INTERLEAVE IN` SQL syntax that allows for good data access patterns for related tables:\n![IMAGE](quiver-image-url/2C899FEB883D910BA3B64A06AA5829B7.jpg =338x290)\n\n#### TrueTime\n\nThe TrueTime API:\n![IMAGE](quiver-image-url/6106A2BEC49644B0448387CEF63926C8.jpg =330x118)\n\nTrueTime relies on two kinds of clocks: `GPS clocks` and `atomic clocks`. Both have different failure modes, so both are maintained for redundancy and fault-tolerance.\n"
    }
  ]
}