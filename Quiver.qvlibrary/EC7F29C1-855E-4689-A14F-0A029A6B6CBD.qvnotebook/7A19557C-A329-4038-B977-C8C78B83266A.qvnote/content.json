{
  "title": "Queueing Theory in Practice",
  "cells": [
    {
      "type": "markdown",
      "data": "[Link to video](https://www.youtube.com/watch?v=Hda5tMrLJqc)\n#### Serial Systems\nTake a concurrent, CPU-bound, low latency API. How do we allocate resources to this properly?\n\nTake a **single-queue / single-queue** model. Say:\n* Tasks arrive independently and randomly at average rate `R`.\n* Server takes time `S` to process each task.\n* Server processes each task one at a time.\n\nSo using some simple geometry with the above variables, you can derive an equation for service latency vs. throughput that maps freakishly well onto actual service latency tests:\n\n![IMAGE](quiver-image-url/BFC310AD7F38EC6FF81FB494403DB499.jpg =1126x644)\n\nGiven that equation, you can see based on time `S` to process each task that improving your service's performance per task will lead to easily the best performance improvement in latency as throughput increases!\n\nThis relies, of course, on a constant time `S` and low variance in arrival rate. We should control these knobs through batching, concurrency control, back-pressure control, fast preemptions, etc.\n\n#### Parallel Systems\nHow about the performance of a **fleet of servers**?\n\nIf each server handles `T` tasks, can `N` servers handle `N * T` tasks? Depends on how we assign tasks - round robin, random, least busy server?\n\nWell, least busy would be nice, but you need to coordinate with your servers (via some routing layer) to figure out what the least busy server is. It takes time to interrogate servers for their load: `N / (aN + S)` is the throughput of concurrent servers, where `N` is number of nodes, `a` is the coordination cost, and `S` is still the time to process a task. It gets worse if the time to coordinate is related to the number of nodes `N`\n\n![IMAGE](quiver-image-url/43AB9BF1BC90F65FA4AD4D646102647D.jpg =1130x644)\n\nSo how can we find the balance between coordination at low parallelism versus high parallelism? Coordination helps for low parallelism, but not for high!\n\nTwo ideas:\n1) Approximate assignment — pick two random servers and choose best. Constant coordination time with scale.\n2) Iterative partitioning — remember the Universal Scalability Law applies to _all_ parallel processes. Process time per node decreases with scale, but coordination time increases. Think about Spark – what if we had a secondary aggregation layer to keep fanout constant per aggregation node, and do aggregation on those intermediate nodes?\n\n![IMAGE](quiver-image-url/986C3EADD727C6F4241A4E78DF11DD38.jpg =1134x648)\n\n![IMAGE](quiver-image-url/D9F39F30B59C7AD18E87C40C8570DCC7.jpg =1866x1052)\n\nWoo! Better! Chief takeaway: coordination has costs, but we can be clever by modelling and looking for better big-O performance."
    },
    {
      "type": "markdown",
      "data": ""
    }
  ]
}