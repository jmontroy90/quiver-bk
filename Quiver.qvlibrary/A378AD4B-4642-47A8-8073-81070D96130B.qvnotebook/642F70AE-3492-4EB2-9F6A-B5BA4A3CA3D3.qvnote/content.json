{
  "title": "Partitioning",
  "cells": [
    {
      "type": "markdown",
      "data": "Partitioning is about scalability. You can partition your data across multiple nodes, such that (hopefully) query-induced load is well-distributed in your data cluster. This stands in contrast to replication, which is just a means to store the same data in multiple places. This can aid in scalability, but has its pitfalls and limits."
    },
    {
      "type": "markdown",
      "data": "### Skew, hot spots, hashing\nPartitioning is usually paired with replication - think of Kafka. Each topic has multiple partitions, and each partition has one leader partition and N in-sync replicas (ISRs) at any given time. Per leader-based replication, your ISRs won't accept writes, but can accept reads.\n\nSay you're partitioning key-value data (like Cassandra / Cosmos). First big problem - **skew**. We want load to be evenly distributed across nodes / partitions, and skewed data + poor partitioning schemes can lead to **hot spots**, where a lot of data access patterns converge on one particular partition (and thus node). It's worth noting that skewed data has a slightly different train of thought to it than bad partitioning schemes (the former is harder to solve).\n\nIf you assign keys randomly and evenly, you'd avoid hot spots from bad partitioning, but then you'd also have to query all nodes / partitions to find the data later. How to do better?\n\nPartitioning with some implicitly ordered key allows for range queries and easy distribution, but also can lead to hot spots if you're careless. Example: timestamps will often lead to hot spots around recent data. Maybe if you partition by source and then timestamp, you'll do better!\n\nHow about **consistent hashing**? If you hash your key and assign the output hash value to a partition, you'll get good distributions. But then you'll lose the ability to query ranges, because the queried column has no direct link to the partition ranges and ordering.\n\n![IMAGE](quiver-image-url/093BE557A1F8CC2770DFACEF26FA0A67.jpg =2880x901)\n\nCassandra manages this trade-off with _compound primary keys_, where the first key column is used for partitioning, and subsequent key columns are for ordering / clustering within a partition. So if you partitioned by company id and then by timestamp for site event logs, you'd be able to range scan on timestamp (essentially batching, remember SSTables!).\n\nAs mentioned earlier, some keys are just hot - a big company, a popular user, etc. In this case, you could arbitrarily prepend some id to the key to break it across partitions, but this is often a manual, post hoc, application-layer operation. **This would be a killer feature for databases of the future to handle!**"
    },
    {
      "type": "markdown",
      "data": "### Secondary Indexes\nHow about **secondary indexes**? They don't lay over partitions at all, so how do we manage? Well, they break into two subcategories:\n1) document-based partitioning\n2) term-based partitioning\n\n##### Document-based partitioning\nThese are **local secondary indexes** — each partition manages its own secondary indexes. This is good if you're already limited to some subset of partitions, but if you query purely on a secondary index, you'll have to do what's called a _scatter-gather_ operation (feels reminiscent of a shuffle) to query all partitions. \n\n![IMAGE](quiver-image-url/A7F709DFB544575884096B9EC2CA43F7.jpg =2880x1365)\n\n##### Term-based partitioning\nThese are **global secondary indexes** — a partitioned, global index is stored throughout your nodes, where the implicit ordering of your secondary key is used to direct your query. This can have some write amplification, and updates to the global index are often asychronous and thus come with usual consistency caveats.\n\n![IMAGE](quiver-image-url/D542D84682593F4E9CF1E7F9F3B393E6.jpg =2880x1223)"
    },
    {
      "type": "markdown",
      "data": "### Rebalancing\nSay you have a scenario like:\n* The query throughput increases, so you want to add more CPUs to handle the load.\n* The dataset size increases, so you want to add more disks and RAM to store it.\n* A machine fails, and other machines need to take over the failed machine’s responsibilities.\n\nYou need to rebalance, i.e. move some data around! A good rebalance should meet these criteria:\n* After rebalancing, the load (data storage, read and write requests) should be shared fairly between the nodes in the cluster.\n* While rebalancing is happening, the database should continue accepting reads and writes.\n* No more data than necessary should be moved between nodes, to make rebalancing fast and to minimize the network and disk I/O load.\n\nBut first - don't use _mod N_ hashing where _N_ is your number of nodes. It'll force data movement when any node is added (`X mod N <> X mod (N + 1)`).\n\n#### Fixed number of partitions\ntl;dr — if you overallocate partitions relative to nodes, it makes it easy for a new node to steal some partitions. This is tough to find the sweet spot — too many fixed partitions, and you've got cumbersome overhead in metadata management. Too few, and moving those bulky partitions around gets costly.\n\n#### Dynamic partitions\nImagine doing a range scan on KV pairs — if you goof your partitioning, you could end up (shocker) hot spots. Dynamic partitioning helps - it could rebalance based on some size threshold of partitions automatically, for example. You could have a problem with a cold start problem of only one partition on one node within a cluster of N nodes, but you could just pre-split in that case (HBase and MongoDB do that).\n\n#### Partitions by node\nYou could just say each new node gets N partitions on it. When a new node comes in, it poaches partitions from other nodes. This is sort of weird to me.\n\nIn general - you should probably have someone on standby for rebalance events. They can be automated, but you better be damn sure."
    },
    {
      "type": "markdown",
      "data": "### Service Discovery\n\nWith all of the above, how does a client know which node to go to get its data, especially since new nodes may be coming in and out + rebalancing? This is called **service discovery**, and there are three basic situations:\n\n* Allow clients to contact any node (e.g., via a round-robin load balancer). If that node coincidentally owns the partition to which the request applies, it can handle the request directly; otherwise, it forwards the request to the appropriate node, receives the reply, and passes the reply along to the client.\n* Send all requests from clients to a routing tier first, which determines the node that should handle each request and forwards it accordingly. This routing tier does not itself handle any requests; it only acts as a partition-aware load balancer.\n* Require that clients be aware of the partitioning and the assignment of partitions to nodes. In this case, a client can connect directly to the appropriate node, without any intermediary.\n\nYou can do this per above, but **gossip protocols** are also possible (Cassandra!). Probablistic convergence to consensus on changes - Paxos, Raft.\n"
    }
  ]
}