{
  "title": "Chapter 9: Data Races",
  "cells": [
    {
      "type": "markdown",
      "data": "### 9.1 - Cache Coherency and False Sharing\nData race - same memory, two goroutines simultaneously, at least one goroutine is writing (dual reads are fine). This is where we need synchronization between threads, where we get things like `atomic` and `mutex`.\n\nAt the hardware level, we deal with value semantics, e.g. cores with their own copy of the data. This can lead to thrashing - each core / goroutine has their own copy, and as a single goroutine modifies some piece of data, other core / cache line copies of that data need to be marked \"dirty\" via snooping protocols (`MESI`), and the newest version of that data fetched via main memory or maybe a shared L3. This is bad.\n\nYou could try to turn your concurrency / synchronization problem and turn it into a parallelization / reduction problem. Say you have unique data per core, maybe you reduce it all later (CRDTs? \"reduce\" in MapReduce?). Each core increments its own data. BUT -- depending on allocation, each core-unique data point might still occupy the same cache line (one slice?), which leads to **false sharing**. The cache line is duplicated across cores, even though each core only needs one data point on that line. This is bad too.\n\n### 9.2 - Synchronization with Atomic Functions\nNo one line of code is atomic (think: how many instructions correspond to this single line of code?). Synchronization in software must be coded explicitly.\n\nYour choices are:\n1) Atomic instructions (small data, hardware-level synchronization)\n2) Mutexes (bigger data, software synchronization)\n\nAtomic requires exact types, and uses good ol' compare-and-swap semantics to perform concurrency.\n> `atomic.AddInt64(&counter, 1)`\n\nIf you want to coordinate multiple lines of code, you need mutexes.\n\n### 9.3 - Synchronization with Mutexes\nMutex == mutual exclusion. It's locking. It's for lines of code, not just counters like the atomic instructions.\n\nFirst - any struct with a mutex as a field cannot be copied (otherwise how you gonna synchronize?). Make sure you share these.\n\nWith a mutex, the scheduler gives the lock to a single goroutine. It's probably not first-come first-serve, but there is logic for fairness.\n\nContended mutexes are a form of internal backpressure!\n\nYou just use `mutex.Lock()` and then the same thread must do `mutex.Unlock()`. Some people use `defer` for this, which is good. \n\nWe can also use `sync.RWMutex` to allow multiple goroutines to take read locks, and one goroutine to write. This is good for things like Maps where many reads need to happen concurrently, but we still need safe access.\n\n### 9.4 - Race Detection\nThe race detector is a great tool, for both `go build -race` and `go test -race`.\n\nA good idea would be to run one QA / PROD-parallel version of your service with the race detector on. At least you should run CI builds with the race detector on.\n\n### 9.5 - Map Data Race\nThe Go language has built-in Map data race detection. Even if you're concurrently modifying separate keys, it's a data race that can lead to corruption. This is an integrity issue prioritized over performance.\n\n### 9.6 - Interface-Based Race Condition\nThis whole exercise shows how single-line assignment isn't necessarily atomic (assign a value + a pointer), and shows how the data race detector can find this. It's like doing dirty reads in a database. Don't get fooled by single-threaded applications either!"
    }
  ]
}