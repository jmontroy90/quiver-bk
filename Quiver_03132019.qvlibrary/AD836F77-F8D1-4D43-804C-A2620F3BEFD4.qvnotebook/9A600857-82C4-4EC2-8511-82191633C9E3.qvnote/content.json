{
  "title": "Big Data Design Patterns (Jet.com)",
  "cells": [
    {
      "type": "markdown",
      "data": "## Intro - 3 minutes\nHi everyone - thanks for coming. My name's John Montroy, I'm a big data software engineer here at Jet.com, been here for close to the last two years. I work here on our big data team, i.e. \"Mystique\" - that's her there. We have a thing with superheroes here at Jet.\n\nSo I've titled my talk today Big Data Design Patterns, and in this talk I'm hoping to convince you that while working with big data is more difficult, it becomes a lot better through the usage of big data design patterns, meant to address the pitfalls of big data systems. \n\nSo here's our agenda for the talk. We'll begin by looking at two ways of storing data, one of them very \"small\" and one of them very \"big\". Through those two technologies, we'll take the time then to introduce some of the concepts and overhead that goes with working with big data systems. Next up we'll move into a couple of big data system designs, starting with something called Command Query Responsibility Segregation and moving into some greek letters. \n\nWhat I'd like for you all to keep in mind as we're talking about the system designs is that these designs are thought up and popularized by people who have worked in the big data space for years. In other words, technically you can just spin up a Hadoop or Cassandra cluster and go on your merry way, but doing that will lead to exactly the problems with data access patterns, growth in complexity, and a lack of operational stability for reprocessing that our design patterns here are supposed to help address. Then once you learn about them abstractly here, you'll learn how we use them (or avoid them!) here at Jet!\n\nOkay, so let's begin.\n\n## Big vs Small\n\nI'd like to introduce big and small data by comparing and contrasting two methods of storing data - one a lowly, humble CSV, and another Apache's Cassandra topic. And in order to compare them, I'm going to tether that to six categories of qualifying data, seen here (read them). It's okay if these words don't mean much to you yet, we'll go through them.\n\nHere we have a simple text file of data, or, more specifically a file of pipe-delimited values (more commonly this would be called a CSV for comma-separated values, which is a misnomer (it should be PSV for pipe-separated values) but everyone knows what you mean). This is probably the simplest way imaginable to represent and store data - as such, I'm not gonna spend too much time going into it.\n\n#### STORAGE\n\nBut let's touch on storage here, how is a simple CSV stored? Well, it's a file on a file system, so that's about it. What's the file system? Who has access to it? Where's the server hosting it? Who knows.\n\nWhat about storage on Cassandra? Let's dive into a few concepts. Looking at the picture on the right here, we see that instead of storing data on one computer like our CSV does, Cassandra has many computers to store data. These computers, or nodes, are organized in a ring as we see here. Now Cassandra partitions our data - so instead of one file storing records for Sam, Alex and John all together, maybe it sends the data for John to node 2b, Sam to 1a and Alex to 1c. So now some user comes in, says give me the data for John, Cassandra says, oh, that's on 2b, here you go! But Cassandra goes even further, right, because what if 2b is down? Well, Cassandra will replicate data to other nodes so it's more available, so if 2b is down, Cassandra will just get it from 1b.\n\nSo Cassandra is a clear win on storage - as we'll go into, higher availability and speedier data access. And if you can manage consistency issues, this storage setup is a clear win. And spoilers - the purpose of the second half of this talk is to show how design patterns help with some of these consistency issues, leaving you with an overwhelming set of pros vs cons on big data.\n\n#### EXECUTION\n\nLet's move on to execution, i.e. how do I work with this file? Uh....Sublime? NotePad++? sed / awk? Python? Microsoft Word? You need external tools, and all those tools have their own methods and opinions. No good.\n\nBut on the Cassandra side - we've got Java drivers and SQL! Standard interfaces with well-defined, documented behaviors!\n\nExecution - Cassandra.\n\n#### STRUCTURE\n\nStructure for a CSV? Well sure, this data representation has structure. Individual records are stored as rows on each line, i.e. they're separated by a newline character; individual columns are columns separated by a pipe. We use normal, human-readable text to write out the records, as opposed to binary or hex or something fancy like protobuf or parquet (more on that later). But this is brittle. For one - what if a record has a newline character in it, literally in the text? A pipe? This'll blow up. What's the character encoding behind this file, for those of you that know that? ASCII, well that umlaut is trouble. this is fragile at best.\n\nSo structure? Well, we've got a proprietary Cassandra data format storing things as individual keys (John, Sam) attached to rows of data, stored on multiple servers. On top of that, we make use of standard, proven data structures - write-ahead logs, SSTables - to store the data! And some of these structure issues for CSVs, like fragile parsing? All taken care of by execution layers, Java drivers. Standard formats, standard interfaces.\n\n#### SPEED / SCALABILITY\n\nIs this all fast? Well, on a CSV - no. You're limited to one data access pattern - row scans. If you want something from column 27 of row 3, you need to go through everything else, i.e. scan the dataset to get there. Worse, you're limited to one node and one set of disk locations - there's gonna be access contention!\n\nInstead, let's try to go towards fast, seek-oriented lookups where load can be scaled by just adding more nodes. That's what Cassandra lets us do - your data is on this node in this partition key range, now just scan through SSTables to get it. And Cassandra offers all these additional data structures to speed things up - Memtables, key and row caches, Bloom filters which aren't even shown here. And we get all this on every replica, and we can just add more and more replicas as load increases!\n\n#### AVAILABILITY\n\nIs it available? Well, let's say this file is stored on a computer somewhere, maybe a server in a data center, maybe in the cloud (so a server in a data center). What if that server is restarted? Goes down? What if someone deletes the file, is there a backup?\n\nOn the Cassandra side - Cassandra replicates all data, so if one node goes down, another can come in to serve the data. Cassandra is also very good at recovering dead nodes, and shifting data around to compensate for dead nodes.\n\n#### CONSISTENCY\n\nAnd last but certainly not least - is it consistent? That means for this - if two people open this file to read the data, are they guaranteed to see the same exact file and data? What if they then edit that data, are they guaranteed to see each other's edits? Well, no - anyone who's ever dealt with unsaved changes in a Word document knows what this might mean. And oddly enough, I'd claim that \"handling unsaved changes\" as just another phrase for \"eventual consistency\". This is a buzzy big data word, and it means - \"in a world where multiple people are doing stuff at the same time on the same data, what will the final resulting data look like, and when will everyone agree on that final data?\". And as the term implies, in big data, we mostly say \"they'll agree eventually\". For a Word doc - that agreement will come when two people working on a doc save their changes and one of them gets a message from Word saying someone else has edited this document while you were working on it, and now you have to deal with that to resolve the differences. Once you do, you and the other person will agree. Another example is working with git - merge your branches with someone else, or with master, and you're finally consistent. \n\nIn the big data world, of course, this is even more complex - we get into things like read repair, last-write-wins, consistently-replicated data types, linearizability and two-phase commit, Paxos and Raft, gossip protols like Scuttlebutt - on and on. What if someone reads the same data from this node versus that node? What if those nodes don't agree yet?  How do we determine what the right data is? How do we deal with network lag, in a world where all node-to-node communication is by definition asynchronous? These problems of consistency are hard, but now we get to the fun part - let's try to use some design patterns to make them easier!\n\n## Design Patterns - Let's Help Ops!\n\nBefore we dive in to the specifics, let's step back and try to name what we're trying to solve here. We've shown that big data systems are great, with a lot of clear advantages especially with availability, scalability, and access patterns. But we've also shown that consistency is hard. Getting separate nodes to agree in the wild is difficult! Another hard problem is reprocessing data - say some bad data comes in, or a node suffers severe latency and yields really inconsistent data. In both situations, some team needs to step in and run some sort of reprocessing, a backfill of data. This is human intervention to solve a fairly standard, name-able problem, and I would sum up these problems - consistency, bad data, backfills - with this: \"How do we manage day-to-day operations in a big data world?\"\n\nTHIS right here is where big data design patterns step in to save the day. They take these operations problems and deal with them explicitly, rather than leaving them as implicit, unspoken issues that demand human intervention at absurd hours. So instead of a team of bright-eyed engineers gathering together every other day to discuss the latest backfill problem, we build a backfill mechanism into our architecture explicitly, and then we're done!\n\nSounds good, right? Let's see how we can do it.\n\n## CQRS - 5 minutes\n\nCQRS, or Command Query Responsibility Separation, is a systems design pattern that pairs nicely with event sourcing / streaming use cases. It's a bit older, but serves as an instructive lead-in to our question: \"How do we do ops?\"\n\nSo to introduce this - let's say we have a UI interacting with an application. We send some data to that application to write to a database, the application storing the data in some model before it goes to the database. Now after that, the normal scenario is to offer CRUD (Create, Read, Update, Delete) semantics on that data, but when we read that data back, we also might transform it in our application - joins, validation, aggregation, and so on. Eventually the data model in our application that represents the writes might wildly diverge from the data model used for reads of the same underlying event -- i.e., the same application needs a very different view for display / reading than for writing / updating. This is where CQRS steps in and says - let's not shoehorn in both the write and read data model into one application, let's separate them! Let's put them in different applications, maybe even different hardware!\n\nSo this image here shows some of the concepts mentioned, as well as their formal terminology - the write data model is the \"Command model\", and the read data model is the \"Query model\".\n\nI want to state here unequivocally that you should be careful about CQRS. This isn't your newest toy to deploy to production just to sound good at conferences or, er, meetups (it's especially not a new toy, since Greg Young first popularized it around 2010, which is like 70 years in software years). You're introducing two models and potentially two different applications - that's a lot of complexity!\n\nWell then - what do we gain from all this? Two things:\n\n1) CQRS does well with high-performance systems. Separating out your read and write data models (and thus, applications) will allow you to optimize for access patterns that more closely fit your needs.  In other words - do one thing well, but multiple times.\n2) CQRS design pairs very nicely with an event sourced architecture - the move away from centralized CRUD operations works well with a microservices, asynchronous, event-sourced world. The lack of centralization also leads pairs with the world of eventual consistency, since your Query and Command models must be kept in sync. \n\nSo between the event sourcing synergy and the high performance benefits, maybe you can see like I do - this leads us towards big data!\n\nAs a side note: this kind of system design actually finds very real corollaries here at Jet.com - as you'll hear later, our production data platform uses Kafka as a message bus, and separates out our raw transformation jobs (what we [and probably lots of people] call projectors) from our aggregation jobs (which we call listeners). In a loose way, our projectors function as the Command layer, with listeners as a Query layer. If you're thinking about lambda and kappa architectures at this point, I agree with you.\n\n#### LAMBDA ARCHITECTURE - 5 minutes\n\nLet's zoom in on one aspect of CQRS here to introduce our next design pattern. CQRS's main conceit was that data models can diverge, especially in a complex, event-sourced system. But if the data models can diverge, can't their access patterns also diverge? Their latencies, their SLAs? What about their ops? Their reprocessing? Bad data? This is the crux of the lambda architecture, a system design popularized by Nathan Marz in a 2011 blog post. His approach comprises four points, the first two of which summarize the \"how\" of the approach - how do we reify reprocessing into a first-class citizen, and the second the \"what\" - what systems do we use to do this?\n\n1) Immutability - The system makes it easy to store and scale an immutable, constantly-growing dataset\n2) Append-only - The primary write operation is adding new immutable facts of data\n3) Batching - The system avoids the complexity of the CAP theorem by recomputing queries from raw data\n4) Streaming - The system uses incremental algorithms to lower the latency of queries to an acceptable level\n\nOr, on the other side of it - lambda architecture seeks to remove mutability and incremental algorithms from your system by using a combination batch + streaming architecture to serve consistent data at low latencies.\n\nOkay, what does all this actually mean? In the wild, it's a two-tiered system. You have a Batch Layer storing immutable, append-only data that computes a query every few hours. Think HDFS and MapReduce piping the query results into some fast KV store like Voldemort. Then, to handle the last few hours in between batch queries, you introduce a Speed Layer that runs an equivalent query in an event processing world. Think Kafka -> Cassandra. Now to get a final, complete piece of information, your application can query both the batch result (Voldemort) and the speed result (Cassandra), combine them, and you have your final result.\n\nWhat do we gain from this? Well:\n\n1) Number one, your batch layer is dead simple. Underlying data is immutable, so views on that data can always be recomputed.\n2) Number two, the complexity of a streaming, eventually consistent system is isolated to your speed layer. Any issues in your speed layer can simply be recomputed and healed by your batch layer the next time the batch query runs. This right here - this handles problems with consistency as well as outages and data problems! These are the problems of big data, so that's exactly what I mean when I said earlier - these patterns help to handle known issues with the big data world.\n3) Number three, we've removed a hard reliance on incremental and / or approximate algorithms that a streaming environment mandates. Some problems just aren't really feasible in a streaming world (distinct counts is one), so by having our whole dataset available for batching, we can move to easier algorithms. \n4) Four, we can also change algorithms and do any ad-hoc analysis at pretty much any time - since the data is persisted and immutable, we can query away! This also helps for schema migrations, which are much harder on the fly. \n5) Last but certainly not least - the lambda architecture realistically tackles one of the hardest, least glamorous problems of big data - our old friends, reprocessing and bad data, i.e. day-to-day operations. People that build streaming systems often don't dedicate too much time to robust reprocessing mechanisms, making migrations and failure recovery difficult. The lambda architecture really takes reprocessing and failure recovery as first principles, first-class citizens, with its emphasis on immutable data and continuous reprocessing. That's good!\n\n## KAPPA ARCHITECTURE\n\nBut hold on a sec - when we discussed CQRS, I went out of my way to caution that CQRS introduces a lot of complexity and should only be used when it absolutely fits the use case. Doesn't the lambda architecture have that problem times ten? We might have different models for reads and writes for every individual view on underlying data, AND in the lambda world, we have two layers for calculating those views! The operational overhead for maintaining two different data processing systems can't be overlooked - it requires deep knowledge of both batch and streaming systems, and how they merge together for a complete dataset. Well, okay, you might say then let's make an abstraction - let's make some data processing system that operates generically on both batch and streaming operations. Beam attempts to do this, as does Spark. But improvements are probably going to be minimal - \"all abstractions are leaky\" is the phrase, right? Indeed, my experience working with Spark's Structured Streaming library has led me to agree with this fact. You write some complex Spark SQL query with joins, and then you have to mentally add in all your knowledge of streaming state, watermarking, stream joins, triggers...it's a lot. AND lambda assumes that your systems actually do perfectly agree, i.e. any iffiness in the speed layer will be totally corrected by the batch layer.  Anyone who's worked a lot on data integrity and validation in distributed systems knows how difficult it can be to root out subtle data inconsistencies born of character encodings, nullability, data types, bad input data with edge cases...\n\nTheese arguments are the ones made by Jay Kreps, founder of Confluent. Apparently, he saw a lot of lambda-esque systems at LinkedIn, and found working with all their components exhausting and error-prone. To counter, he challenged the idea that stream processing was \"inappropriate\" for reprocessing huge amounts of data (normally a batch task) by noting (and I think this really sums it up well): \"Stream processing is just a generalization of this [batch] data-flow model that exposes checkpointing of intermediate results and continual output to the end user.\"\n\nHis basic idea is dead simple - instead of relying on batch computations for view computation, query latency reduction, and error correction, just stream all the data really, really fast. That's it. If you want to migrate schema, or compute a new view with more columns, just modify your original code, run all your data through the modification really fast (while the original job is still running), and then once you're caught up, switch any upstream dependencies to read from your new data. He called this kind of system the Kappa architecture, in honor of lambda.\n\nIs this better? Well, we kinda hope so. This is more or less what we do here! So I'm gonna let Alex speak a bit more to how we do it, and what the complexities of it are. In short though - reprocessing and error handling takes time in this world. It's doable, but resource intensive! Is that okay? It's certainly better than ignoring it completely!"
    }
  ]
}