{
  "title": "Structured Streaming Jet POC - Notes and Comments",
  "cells": [
    {
      "type": "markdown",
      "data": "### Current POC Framework\n\nRepo: `mystique-parent`\nBranch: `MYS-745-structured-streaming-poc`\nCode is in: `com.jet.mystique.adhoc.structured`\nMain test command: `sbt -jvm-debug 5005 \"gorillaGroddTesting/test-only *BatmanRviSpec -- -oF`\n\n\n#### Verdict (for now)\n\nIt does solve a certain set of problems, but not many of those problems are our problems right now. It _is_ a better, faster, cleaner API, and we should probably move our normal projectors to it anyway.\n\nNext steps:\n* Get an understanding of the StateStore memory size per executor, ie. load testing. \n* Clean up the API and generalize it.\n\n#### Features\n1) Can use either `TableSink` for Cassandra or the `CassandraForEach`, injecting the choice at the top level.\n    1) `TableSink` allows us to work with Cassandra via Spark (our current standard method), but seems to incur a `collect` of incremental RDDs to the driver. The existing state is distributed in state stores on each executor and is updated per partition.\n    2) `CassandraForEach` avoids the seemingly necessary `collect`, but requires us to add code working directly with a Cassandra `Session` instead of using Spark. Akin to the SQL Server \"query serialization\" we used to do. [Read this section completely for successful ForeachWriter usage](https://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/structured-streaming-programming-guide.html#using-foreach).\n2) Can inject the **output mode** (complete, append), the **trigger window** (1 minute, 5 minute), and the **aggregation window** (1 minute, 5 minute) at the top-level object (which is what I do for easy testing). Would be easy to add injection points for the sliding window and for the watermarks.\n    1) Testing watermarking deterministically, though, seems tricky. Crux: event time vs business time?\n3) Can dump input data into Kafka and sleep appropriately in the unit tests to see how state aggregates and gets pushed to Cassandra. Testing stream timeout is managed via `terminateTime`.\n4) Has basic hooks for using existing `selectExpressions`, `jsonSchemaSpec`, `GorillaGroddTable`, and `changeStreamTopic` testing.\n\n#### NOT Features\n1) Arbitrary stateful computations via `mapGroupsWithState`.\n2) Spark 2.2+ features, including:\n    1) `KafkaSink` — 2.1 has `KafkaSource`\n    2) The `update` output mode — would reduce state stored per executor and driver.\n\n#### Questions and answers\n1) Can we do multiple aggregates across multiple time windows?\n    1) Yes, with `mapGroupsWithState`. Not so easily with normal aggregates.\n2) What sort of watermarking does an aggregate get?\n    1) \n3) How does checkpointing work with WASB?\n    1) Just fine!\n4) Does each executor hold its own slice of the state, or is it all stored in the driver or something?\n    1) See section on `StateStore` below. Each executor has its own `StateStore` which is updated per partition. I don't have ALL the details solid, but I believe the aggregated state is flushed to a sink on a per-partition basis.\n5) How are checkpoints read and executors bootstrapped on a restart before actual processing resumes?\n    1) We can run a demo on what the checkpoint directory looks like.\n\n#### TODOS:\n* `ProjectorSet` slightly decomposed to allow for stream processing without mappings or Kafka. To be reconciled with Alex's work.\n* Small fork from the `DStreamOffsetKafkaProjectorSet` into a new `StructuredOffsetKafkaProjector`. Only using one `Projector` for now. Is this okay?\n* Config hard-coded for input consumer. Need to reconcile with `MystiqueConsumer`?\n* `processStream` returns `unit` and I need `processStructuredStream` to return something for flexibility.\n* Watermarking for testing is not fully understood, and wasn't tested in QA.\n\n#### StateStore\n\nUsed to store state. One per executor, coordinated by a `StateStoreCoordinator` on the driver. State is updated given an input RDD per partition, using the state store of the executor the partition lives on.\n\n[Original design review here](https://docs.google.com/document/d/1-ncawFx8JS5Zyfq1HAEGBx56RDet9wfVp_hDM8ZL254/edit)\n\nEntry point for Structured Streaming:\n1) Start a StreamingQuery\n2) Uses a StreamingQueryManager\n3) has a StateStoreCoordinator with the sparkContext.env passed in\n4) env is set by createDriverEnv\n\nThe actual `start()` call on a `StreamingQuery` invokes `addBatches()`, which is where an `IncrementalRDD` is generated for reconciling the current batch with existing state stored on each executor's `StateStore`.\n\nThe `IncrementalRDD` comes down to a `mapPartitionsWithStateStore`, which uses a `StateStoreRDD`. The method `compute` on that takes the input RDD and applies a function `storeUpdateFunction: (StateStore, Iterator[T]) => Iterator[U]` to it to get the latest state.\n\nThat reconciled state is stored in the per-executor `StateStore` as a `HashMap`.\n\nState stores use some persistent store to checkpoint for fault tolerance. The most immediate example is the `HDFSBackedStateStore`. `StateStore` is checkpointed and serialized on disk using Kryo."
    }
  ]
}